# Application of Reinforcement Learning Methods

This repository implements a set of reinforcement learning algorithms to run on OpenAI gym environments.
The algorihtms implemented are:
- [Relative Entropy Policy Search (REPS)](https://www.ias.informatik.tu-darmstadt.de/uploads/Team/JanPeters/Peters2010_REPS.pdf)
- [Actor-Critic Relative Entropy Policy Search (ACREPS)](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12247)
- [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)

In practice they were only tested on the following environments: \
`Pendulum Swingup`, `Double Cartpole`, `Furuta Pendulum`, `Ball Ballancer` \
The last three are custom gym environments implemented in the quansar_robots repository.

## Repository Contents
1. `run.py` main entry point for all training and evaluation tasks.
2. `experiments.py` convenience method to run multiple experiments with the same settings in parallel processes.
3. `/agents`
    * `acreps.py` Actor-Critic Relative Entropy Policy Search
    * `reps.py` Relative Entropy Policy Search
    * `ppo.py` Proximal Policy Optimization
4. `/common` common code
5. `/hyperparameters` good known hyperparameters for algorithms
6. `/out` contains all outputs generated by the training process

## Training and Running Models
The main entry point of this repository is the `run.py` file. It comes with a sophisticated command-line parser and
special subparsers for each algorithm implemented in this repository.
The basic syntax is:
```
python run.py [general arguments] (ACREPS|REPS|PPO) [algorithm specific arguments]
```
More information on required and on optional commands can be explored by running `python run.py -h`.
Which returns
```
python run.py [-h] --name NAME --env ENV [--robot] [--n_epochs N_EPOCHS]
              [--n_steps N_STEPS] [--seed SEED] [--render] [--experiment]
              [--eval | --resume]
              {REPS,ACREPS,PPO}
```
For information on algorithm specific commands the ``-h`` can executed on the subcommands `{ACREPS,REPS,PPO}`,
e.g. ``python run.py REPS -h`` to get more information training the REPS algorithm.

The most basic command for running REPS on the underactuated pendulum swingup would be
```
python run.py --name reps_pendulum --env pendulum REPS
```

#### Training with Custom Hyperparameter Settings
By default a default set of hyperparameters is loaded from `hyperparameters/[algorithm]/[environoment].yaml`.
But each of the algorithms' sub commands `(REPS|ACREPS|PPO)` can take custom hyperparameter commands.
To figure out the available hyperparameters that can be set the `-h` flag can be run on the algorhtms subcommand,
e.g. `python run.py REPS -h` returns:
```
usage: run.py REPS [-h] [--epsilon EPSILON] [--gamma GAMMA]
                   [--n_fourier N_FOURIER]
                   [--fourier_band FOURIER_BAND [FOURIER_BAND ...]]

optional arguments:
  -h, --help            show this help message and exit
  --epsilon EPSILON     KL constraint.
  --gamma GAMMA         1 minus environment reset probability.
  --n_fourier N_FOURIER
                        number of fourier features.
  --fourier_band FOURIER_BAND [FOURIER_BAND ...]
                        number of fourier features.
```
Training REPS on the pendulum with a cusom gamma and more fourier features is as easy as
```
python run.py --name reps_pendulum --env pendulum REPS --gamma 0.9 --n_fourier 200
```

#### Resume Training
Experiments can be stopped during the training process and resumed afterwards by using the `--resume` flag.
TODO: more explaination

#### Evaluating Experiments
Trained Models can be evaluated using the `--eval` flag.
TODO: more explaination

#### Visualising Training with TensorBoard
Different scalar values that occure during the training process are saved into tensorboard files automatically.
These files are either located in `out/summary/[experiment_name]` or in `out/experiments/[experiment_name]/summary`
depending on whether the experiment was invoced via `run.py` or `experiments.py`. Usually however experiemnts will
be located in the first of both locations.
To start a tensorboard that loads all experiments invoced via `run.py` just execute:
```
tensorboard --logdir=out/summary
```

## Installation

In order to run the code provided, follow these steps to install the necessary components.

### Conda environment

In order to manage the different python versions we use conda for creating virtual environments. First, get mini conda from <https://conda.io/en/latest/miniconda.html> and install it. Next, create an empty virtual conda environment with Python 3. Create the environment by executing:

```conda create --name rl-env python=3.6.5```

and then activate it by executing

```conda activate rl-env```

**IMPORTANT: When not using a virtual environment, use *pip3* instead of *pip* in the instructions below.**

### Quanser Robots
Install the modified OpenAI gym environments by first cloning the git repository

```git clone https://git.ias.informatik.tu-darmstadt.de/quanser/clients.git```

and then install by executing

```
cd clients
pip install -e .
```

### Pytorch framework
As we used the pytorch framework (<https://pytorch.org/#pip-install-pytorch>) install the appropriate version:

For mac:

```pip install torch torchvision```

For Linux (non-gpu version):

```
pip install https://download.pytorch.org/whl/cpu/torch-1.0.0-cp36-cp36m-linux_x86_64.whl
pip install torchvision
```

### Tensorflow and TensorboardX

In order to install tensorflow (non-gpu version), execute:

```pip install tensorflow```

For evaluation and visualization of the learning, install tensorboardX, which is a tensorboard port for Pytorch:

```pip install tensorboardX```

### Other requirements
- joblib
- yaml
- ...

### Installing the learning algorithms

```git clone https://github.com/danielpalen/rl-research-lab-class.git```

Now you are all set to train a model or evaluate an algorithm.
